{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from class_9_3  import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同字符数量\n",
    "CHAR_SET_LEN = 10\n",
    "# 图片高度\n",
    "IMAGE_HEIGHT = 60\n",
    "# 图片宽度\n",
    "IMAGE_WIDTH = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义卷积神将网络必要函数\n",
    "# \n",
    "# 初始化权重\n",
    "def weigth_variable(shape):\n",
    "    initial = tf.truncated_normal(shape=shape, mean=0, stddev=1.0) # 生成阶段正态分布数据\n",
    "    return tf.Variable(initial_value=initial, dtype=tf.float32)\n",
    "\n",
    "# 初始化偏置\n",
    "def biase_variable(shape):\n",
    "    initial = tf.constant(value=0.1, dtype=tf.float32)\n",
    "    return tf.Variable(initial_value=initial, dtype=tf.float32)\n",
    "\n",
    "# 卷积层\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(input=x, filter=W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# 池化层\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(value=x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# 正则化\n",
    "def regularizer(value):\n",
    "    regularizer = tf.contrib.layers.l1_l2_regularizer()\n",
    "    return tf.get_variable(initializer=value, regularizer=regularizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络模型结构\n",
    "# \n",
    "# 第一层 输入层\n",
    "# \n",
    "# 初始化两个 placeholder 作为输入 images 、labels\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(None, 224, 224, 1))\n",
    "# 多任务\n",
    "y0 = tf.placeholder(dtype=tf.float32, shape=(None, 10))\n",
    "y1 = tf.placeholder(dtype=tf.float32, shape=(None, 10))\n",
    "y2 = tf.placeholder(dtype=tf.float32, shape=(None, 10))\n",
    "y3 = tf.placeholder(dtype=tf.float32, shape=(None, 10))\n",
    "\n",
    "# 隐层层\n",
    "# \n",
    "# 一\n",
    "# \n",
    "# 初始化卷积核权重\n",
    "W_conv1 = weigth_variable(shape=(3, 3, 1, 32)) # shape=(height, width, channel, filter_num)\n",
    "# 为每一个卷积核初始化一个偏置\n",
    "B_conv1 = biase_variable(shape=(1, 32)) # 为卷积后的 256 个特征图，每一个图加一个偏置 [[1, 2, 3], [4, 5, 6]] + 1 = [[2, 3, 4], [5, 6, 7]]\n",
    "# 激活函数 relu\n",
    "h_conv1 = tf.nn.relu(features=(conv2d(x=x, W=W_conv1) + B_conv1))\n",
    "# 池化\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# 二\n",
    "# \n",
    "# 初始化卷积核权重\n",
    "W_conv2 = weigth_variable(shape=(3, 3, 32, 64)) # channel 是由上一层 filter_num 决定的\n",
    "# 为每一个卷积核初始化一个偏置\n",
    "B_conv2 = biase_variable(shape=(1, 64))\n",
    "# 激活函数 relu\n",
    "h_conv2 = tf.nn.relu(features=(conv2d(x=h_pool1, W=W_conv2) + B_conv2))\n",
    "# 池化\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# 三\n",
    "# \n",
    "# 初始化卷积核权重\n",
    "W_conv3 = weigth_variable(shape=(3, 3, 64, 128)) \n",
    "# 为每一个卷积核初始化一个偏置\n",
    "B_conv3 = biase_variable(shape=(1, 128))\n",
    "# 激活函数 relu\n",
    "h_conv3 = tf.nn.relu(features=(conv2d(x=h_pool2, W=W_conv3) + B_conv3))\n",
    "# 池化\n",
    "h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "# 四\n",
    "# \n",
    "# 初始化卷积核权重\n",
    "W_conv4 = weigth_variable(shape=(3, 3, 128, 256)) \n",
    "# 为每一个卷积核初始化一个偏置\n",
    "B_conv4 = biase_variable(shape=(1, 256)) \n",
    "# 激活函数 relu\n",
    "h_conv4 = tf.nn.relu(features=(conv2d(x=h_pool3, W=W_conv4) + B_conv4))\n",
    "# 池化\n",
    "h_pool4 = max_pool_2x2(h_conv4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全连接层\n",
    "# \n",
    "# 扁平化处理\n",
    "h_flat = tf.reshape(tensor=h_pool4, shape=(-1, 14 * 14 * 256)) # 卷积不改变 image 尺寸，因为 padding='same'。每一池化 image 尺寸缩小 1 倍\n",
    "\n",
    "# 第一层全连接\n",
    "#\n",
    "# 初始化全连接权重\n",
    "W_fc1 = weigth_variable(shape=(14 * 14 * 256, 1024)) # 设置全连接 1024 个神经元\n",
    "# 偏置\n",
    "B_fc1 = biase_variable(shape=(1, 1024))\n",
    "# 激活函数\n",
    "h_fc1 = tf.nn.relu(features=(tf.matmul(a=h_flat, b=W_fc1) + B_fc1))\n",
    "\n",
    "# 第二层全连接（最后一层分类层，多任务模式）\n",
    "W_fc2 = weigth_variable(shape=(1024, 10))\n",
    "B_fc2 = biase_variable(shape=(1, 10))\n",
    "\n",
    "# 激活函数（使用 softmax 作为分类器）\n",
    "# \n",
    "# 任务 0\n",
    "predictions_0 = tf.nn.softmax(logits=(tf.matmul(a=h_fc1, b=W_fc2) + B_fc2))\n",
    "# 任务 1\n",
    "predictions_1 = tf.nn.softmax(logits=(tf.matmul(a=h_fc1, b=W_fc2) + B_fc2))\n",
    "# 任务 2\n",
    "predictions_2 = tf.nn.softmax(logits=(tf.matmul(a=h_fc1, b=W_fc2) + B_fc2))\n",
    "# 任务 3\n",
    "predictions_3 = tf.nn.softmax(logits=(tf.matmul(a=h_fc1, b=W_fc2) + B_fc2))\n",
    "\n",
    "\n",
    "# 打印 y 、 predictions 形状\n",
    "# print(y0.shape)\n",
    "# print(predictions_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数（使用交叉熵损失函数）\n",
    "# \n",
    "# 任务 0\n",
    "cross_entropy_0 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y0, logits=predictions_0))\n",
    "# 任务 1\n",
    "cross_entropy_1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y1, logits=predictions_1))\n",
    "# 任务 2\n",
    "cross_entropy_2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y2, logits=predictions_2))\n",
    "# 任务 3\n",
    "cross_entropy_3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y3, logits=predictions_3))\n",
    "\n",
    "# 4 个任务总损失\n",
    "total_loss = (cross_entropy_0 + cross_entropy_1 + cross_entropy_2 + cross_entropy_3) / 4.0\n",
    "\n",
    "# 定义优化器（联合训练，使用一个优化器）\n",
    "# \n",
    "# 定义可变学习率\n",
    "global_step = tf.Variable(initial_value=0, dtype=tf.float32, trainable=False) # trainable=False 防止被数据流图收集，在训练时尝试更新\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=0.1, global_step=global_step, decay_steps=50, decay_rate=0.96 )\n",
    "train = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss=total_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算任务准确率（四个任务分开计算）\n",
    "# \n",
    "# 任务 0\n",
    "correct_predictions_0 = tf.equal(x=tf.argmax(input=y0, axis=1), y=tf.argmax(input=predictions_0, axis=1)) # 函数 tf.argmax()，默认 axis=0 ，获取'第一维'最大值的索引\n",
    "# 任务 1\n",
    "correct_predictions_1 = tf.equal(x=tf.argmax(input=y1, axis=1), y=tf.argmax(input=predictions_1, axis=1))\n",
    "# 任务 2\n",
    "correct_predictions_2 = tf.equal(x=tf.argmax(input=y2, axis=1), y=tf.argmax(input=predictions_2,axis=1))\n",
    "# 任务 3\n",
    "correct_predictions_3 = tf.equal(x=tf.argmax(input=y3, axis=1), y=tf.argmax(input=predictions_3, axis=1))\n",
    "\n",
    "accuracy_0 = tf.reduce_mean(input_tensor=tf.cast(x=correct_predictions_0, dtype=tf.float32))\n",
    "accuracy_1 = tf.reduce_mean(input_tensor=tf.cast(x=correct_predictions_1, dtype=tf.float32))\n",
    "accuracy_2 = tf.reduce_mean(input_tensor=tf.cast(x=correct_predictions_2, dtype=tf.float32))\n",
    "accuracy_3 = tf.reduce_mean(input_tensor=tf.cast(x=correct_predictions_3, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化变量\n",
    "init_op = tf.global_variables_initializer()\n",
    "# 迭代次数\n",
    "epochs = 2\n",
    "# 用于保存模型\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算图\n",
    "with tf.Session() as sess:\n",
    "    # 获取样本数据、标签\n",
    "    features = read_and_decode()\n",
    "    image_batch, label0_batch, label1_batch, label2_batch, label3_batch = image_labels_to_tensor(features=features)\n",
    "    # 创建 writer，写入日志文件\n",
    "    writer = tf.summary.FileWriter('logs/', tf.get_default_graph())\n",
    "\n",
    "    # 初始化变量\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 创建一个协调器，管理线程\n",
    "    coord = tf.train.Coordinator()\n",
    "    # 启动 QueueRunner，此时文件名文件名队列已经进队\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 获取样本数据、标签\n",
    "        train_image, train_label0, train_label1, train_label2, train_label3 = sess.run(fetches=[image_batch, label0_batch, label1_batch, \n",
    "                                                                                                label2_batch, label3_batch])\n",
    "        # 训练、计算 loss\n",
    "        _, loss = sess.run(fetches=[train, total_loss], feed_dict={x: train_image, y0: train_label0, \n",
    "                                                                   y1: train_label1, y2: train_label2, y3: train_label3})\n",
    "        # 准确率\n",
    "        acc0, acc1, acc2, acc3 = sess.run(fetches=[accuracy_0, accuracy_1, accuracy_2, accuracy_3], \n",
    "                                          feed_dict={x: train_image, y0: train_label0, y1: train_label1, \n",
    "                                                     y2: train_label2, y3: train_label3})\n",
    "        lr = sess.run(learning_rate)\n",
    "        print('Iter %d  Loss: %.4f  Accuracy: [%.3f, %.3f, %.3f, %.3f]  Learning_rate: %.4f'%(i, loss, acc0, acc1, acc2, acc3))\n",
    "        \n",
    "    writer.close()\n",
    "    saver.save(sess=sess, save_path='model/crack_captcha.model')\n",
    "        \n",
    "    # 通知其他线程关闭\n",
    "    coord.request_stop()\n",
    "    # 其他线程关闭，这一函数才能返回\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
